---
title: Attention mechanism 
description: This article explains the attention mechanism in a simple way.
lead: This article explains the attention mechanism in a simple way.
date: 2022-01-24T14:00:00.000Z
tags:
  - "New Post"
authorbox: false
sidebar: false
pager: false
draft: true
---
Image you want to translate the following sentence from English to French: AI is truly an amazing technology. In this day and age, you have two options. Either you take a one semester course in French, learn how to say baguette and croissant and try to translate the above sentene. More likely than not, you will fail, in which case you will open an LLM terminal and ask an AI do it for you. In this blog, I will not give you French lessons. We will try to understand what the LLM did for you, step by step. We will follow what happens exactly inside the attention mechanism, and along the way we will explain in details each component and its role. Let's start wit the very step, tokenization.  

## Tokenization

Conputers don't understand words, they can only read binaries, that is 0's and 1's, which themself translate into a series of electric signals inside the billions of transitors that make up modern computers (if you are using an Apple M2, there are about 13 billions of them).  
## Attention

## Self-Attention

## Multi-Head Attention