<!DOCTYPE html>
<html class="no-js" lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Tokenization - Ismail, Abouamal</title>
    <script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
    <meta name="description" content="This article explains the attention mechanism in a simple way.">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="dns-prefetch" href="//fonts.googleapis.com">
    <link rel="dns-prefetch" href="//fonts.gstatic.com">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

    <link rel="stylesheet" href="/css/style.css">
    

    <link rel="shortcut icon" href="/favicon.ico">
        

    
	<script>
		window.MathJax = {

		  tex: {
			packages: {'[+]': ['amsmath']},
			inlineMath: [['$', '$'], ['\\(', '\\)']],
			displayMath: [['$$', '$$'], ['\\[', '\\]']]
		  },
		  startup: {
			ready: () => {
			  MathJax.startup.defaultReady();
			  console.log('MathJax is ready!');
			}
		  }
		};
	</script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo logo--mixed">
		<a class="logo__link" href="/" title="Ismail, Abouamal" rel="home">
			<div class="logo__item logo__imagebox">
					<img class="logo__img" src="/img/logo_blue.png">
				</div><div class="logo__item logo__text">
					<div class="logo__title">Ismail, Abouamal</div>
					<div class="logo__tagline">Personal blog</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">

		<li class="menu__item">
			<a class="menu__link" href="/">
				
				<span class="menu__text">Blog</span>
				
			</a>
		</li>

		<li class="menu__item">
			<a class="menu__link" href="/about/">
				
				<span class="menu__text">About me</span>
				
			</a>
		</li>

		<li class="menu__item">
			<a class="menu__link" href="/publications/">
				
				<span class="menu__text">Publications</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Tokenization</h1>
			<p class="post__lead">This article the BPE algorithm though an easy example.</p>
			
		</header>
		<div class="content post__content clearfix">
			<p>Suppose you want to translate the following sentence from English to French: &ldquo;The world is amazing.&rdquo; In todayâ€™s digital age, you would likely open a large language model (LLM) and ask the AI to perform the translation for you. In this post, we will explore the very first step that the LLM takes to do the job, namely tokenization.</p>
<p>Computers don&rsquo;t understand words; they can only read binary, which consists of a series of electrical signals within the billions of transistors that make up modern computers (for example, an Apple M2 contains about 13 billion transistors). The job of a tokenizer is to transform our initial sentence into a series of numbers that can be read by the computer. There are many ways to construct a tokenizer, but the most straightforward approach is to encode each character of the English language. However, this method results in very long sequences of numbers, which is computationally inefficient; we want to minimize the number of computations, and having a lengthy sequence of numbers to process won&rsquo;t help! Thus, we need to consider an alternative. A more efficient way to encode a sequence is through Byte Pair Encoding (BPE), which was introduced in 1994 by Philip Gage as a text compression algorithm.</p>
<p>The first ingredient of the BPE algorithm is a corpus of text. Let&rsquo;s take the following example.</p>
<pre><code>                                          The world is full of wonders.  
                                          Mountains and oceans,  
                                          beauty everywhere.  
                                          The world is amazing!
</code></pre>
<p>We begin by preprocessing the text. This involves converting the entire corpus to lowercase, removing punctuation marks, and replacing spaces with underscores to indicate the end of a word.</p>
<pre><code>                                          [t, h, e, _, 
                                          w, o, r, l, d, _, 
                                          i, s, _, 
                                          f, u, l, l, _, 
                                          o, f, _, 
                                          w, o, n, d, e, r, s, _, 
                                          m, o, u, n, t, a, i, n, s, _, 
                                          a, n, d, _, 
                                          o, c, e, a, n, s, _, 
                                          b, e, a, u, t, y, _, 
                                          e, v, e, r, y, w, h, e, r, e, _, 
                                          t, h, e, _, 
                                          w, o, r, l, d, _, 
                                          i, s, _, 
                                          a, m, a, z, i, n, g, _]
</code></pre>
<p>Out basic dictionary consists of the set of characters used in out corpus. In this case, it is given by 22 tokens.</p>
<pre><code>{'i': 0, 'w': 1, 'r': 2, 'z': 3, 'e': 4, 's': 5, 't': 6, 'd': 7, 'y': 8, 'l': 9, 'a': 10, 'f': 11, 'v': 12, 'h': 13, '_': 14, 'c': 15, 'b': 16, 'g': 17, 'u': 18, 'm': 19, 'n': 20, 'o': 21}
</code></pre>
<p>Using the BPE algorithm, we will expand the size of our vocabulary. First, let&rsquo;s count the occurrences of consecutive character pairs in our corpus. For example, the pair <code>(t, h)</code> occurs twice, while the pair <code>(c, e)</code> occurs once. We identify the pair with the highest count, which in this case is <code>(s, _ )</code>, occurring 5 times. We will add this pair to our vocabulary, increasing the total number of tokens to 23. Next, we will revisit our corpus and merge every occurrence of <code>s</code> followed by <code>_</code>. This results in:</p>
<pre><code>                                          [t, h, e, _, 
                                          w, o, r, l, d, _, 
                                          i, s_, 
                                          f, u, l, l, _, 
                                          o, f, _, 
                                          w, o, n, d, e, r, s_, 
                                          m, o, u, n, t, a, i, n, s_, 
                                          a, n, d, _, 
                                          o, c, e, a, n, s_, 
                                          b, e, a, u, t, y, _, 
                                          e, v, e, r, y, w, h, e, r, e, _, 
                                          t, h, e, _, 
                                          w, o, r, l, d, _, 
                                          i, s_, 
                                          a, m, a, z, i, n, g, _]
</code></pre>
<p>Now we will repeat the exact same algorithm by counting consecutive pairs, taking the one that appears most frequently, adding it to the vocabulary, and then merging the corpus again. This process continues until we reach the desired vocabulary size, which is a hyperparameter that controls the size of the final vocabulary. If we run the BPE algorithm with 5 merging cycles, the final vocabulary is:</p>
<pre><code>{'i': 0, 'w': 1, 'r': 2, 'z': 3, 'e': 4, 's': 5, 't': 6, 'd': 7, 'y': 8, 'l': 9, 'a': 10, 'f': 11, 'v': 12, 'h': 13, '_': 14, 'c': 15, 'b': 16, 'g': 17, 'u': 18, 'm': 19, 'n': 20, 'o': 21, ('s', '_'): 22, ('h', 'e'): 23, ('w', 'o'): 24, ('d', '_'): 25, ('t', 'he'): 26}
</code></pre>
<p>Now our initial sequence, &ldquo;wow, the world is amazing,&rdquo; is encoded as:</p>
<pre><code>['wo', 'w', '_', 'the', '_', 'wo', 'r', 'l', 'd_', 'i', 's_', 'a', 'm', 'a', 'z', 'i', 'n', 'g', '_']
</code></pre>
<p>As you can see, since <code>&quot;the&quot;</code>, <code>&quot;wo&quot;</code> and <code> &quot;d_&quot;</code> appear quite frequently in our corpus, the BPE algorithm included those in the vocabulary as tokens, which is more efficient than trying to assign a code to each character.</p>
<p>You can download a full implementation of the BPE algorithm in <a href="https://github.com/ismaiill/BPE_tokenizer">this repository.</a></p>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M4 0h8s2 0 4 2l15 15s2 2 0 4L21 31s-2 2-4 0L2 16s-2-2-2-4V3s0-3 4-3m3 10a3 3 0 0 0 0-6 3 3 0 0 0 0 6"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/llms/" rel="tag">LLMs</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 Ismail, Abouamal.
			<span class="footer__copyright-credits">Powered by <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a>.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>